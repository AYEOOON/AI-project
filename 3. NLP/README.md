# NLP(Natural Language Processing)
인공지능의 한 분야로서 머신러닝을 사용하여 텍스트와 데이터를 처리하고 해석

### 자연어 처리 및 자연어 처리 Task 조사
https://www.notion.so/686277d06e1a49809078806636c3ed5f
##### 자연어 처리에 대한 정의와 원리, 사용분야를 정리

##### 크롤링 실습하기

- 식신로드 서울지역 '만점 식당' 20선 뽑아오기

## 영어/한국어 Word2Vec 실습
#### gensim 패키지에서 제공하는 이미 구현된 Word2Vec을 사용하여 영어와 한국어 데이터를 학습

##### 1️. 훈련 데이터 이해하기
- 자연어를 얻기 위해서는 전처리가 필요하다.
- 실질적 데이터는 영어문장으로만 구성된 내용을 담고 있는 <content>와 </content> 사이의 내용이다.
- 전처리 작업을 통해 xml 문법들은 제거하고, 해당 데이터만 가져와야 한다.


##### 2️. 훈련 데이터 전처리하기
- 정규표현식을 이용하여 훈련에 필요한 데이터만 정제


##### 3️. Word2Vec 훈련시키기
- Word2Vec의 하이퍼파라미터값은 다음과 같습니다.

vector_size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.

window = 컨텍스트 윈도우 크기

min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)

workers = 학습을 위한 프로세스 수

sg = 0은 CBOW, 1은 Skip-gram.


##### 4️. Word2Vec 모델 저장하고 로드하기
- 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장

#### 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개
자연어 처리 작업을 할때, 케라스의 Embedding()를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용 할 수도 있습니다.


예를 들어서 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황이라면, 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 때로는 더 좋은 성능을 얻을 수 있습니다.


## 네이버 영화 리뷰 감성 분류하기(Naver Movie Review Sentiment Analysis)
#### 해당 데이터를 다운로드 받아 감성 분류를 수행하는 모델을 만들어보는 실습
총 200,000개 리뷰로 구성된 네이버 영화 리뷰 데이터로 영화 리뷰에 대한 텍스트와 해당 리뷰가 긍정인 경우 1, 부정인 경우 0을 표시한 레이블로 구성되어져 있습니다. 

##### 1. 네이버 영화 리뷰 데이터에 대한 이해와 전처리
1) 데이터 로드하기
2) 데이터 정제하기
3) 토큰화
4) 정수 인코딩
5) 빈 샘플(empty samples) 제거
6) 패딩

##### 2. LSTM으로 네이버 영화 리뷰 감성 분류하기
하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다. 모델은 다 대 일 구조의 LSTM을 사용합니다. 

##### 3. 리뷰 예측해보기
임의의 리뷰에 대해서 예측하는 함수를 만들어보겠습니다. 기본적으로 현재 학습한 model에 새로운 입력에 대해서 예측값을 얻는 것은 model.predict()를 사용합니다. 그리고 model.fit()을 할 때와 마찬가지로 새로운 입력에 대해서도 동일한 전처리를 수행 후에 model.predict()의 입력으로 사용해야 합니다.


## 네이버 쇼핑 리뷰 감성 분류하기(Naver Shopping Review Sentiment Analysis)
##### 1. Colab에 Mecab 설치
여기서는 형태소 분석기 Mecab을 사용합니다. 

##### 2. 네이버 쇼핑 리뷰 데이터에 대한 이해와 전처리
1) 데이터 로드하기
2) 훈련 데이터와 테스트 데이터 분리하기
3) 레이블의 분포 확인
4) 데이터 정제하기
5) 토큰화
6) 단어와 길이 분포 확인하기
7) 정수 인코딩
8) 패딩

##### 3. GRU로 네이버 쇼핑 리뷰 감성 분류하기
하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다. 모델은 다 대 일 구조의 LSTM를 사용합니다. 

##### 4. 리뷰 예측해보기
임의의 문장에 대한 예측을 위해서는 학습하기 전 전처리를 동일하게 적용해줍니다. 전처리의 순서는 정규 표현식을 통한 한국어 외 문자 제거, 토큰화, 불용어 제거, 정수 인코딩, 패딩 순입니다.
