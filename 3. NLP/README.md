# NLP(Natural Language Processing)
#### 인공지능의 한 분야로서 머신러닝을 사용하여 텍스트와 데이터를 처리하고 해석

### 자연어 처리 및 자연어 처리 Task 조사
https://www.notion.so/686277d06e1a49809078806636c3ed5f
###### 자연어 처리에 대한 정의와 원리, 사용분야를 정리

###### 크롤링 실습하기

- 식신로드 서울지역 '만점 식당' 20선 뽑아오기

### 영어/한국어 Word2Vec 실습
###### gensim 패키지에서 제공하는 이미 구현된 Word2Vec을 사용하여 영어와 한국어 데이터를 학습

1️⃣ 훈련 데이터 이해하기
- 자연어를 얻기 위해서는 전처리가 필요하다.
- 실질적 데이터는 영어문장으로만 구성된 내용을 담고 있는 <content>와 </content> 사이의 내용이다.
- 전처리 작업을 통해 xml 문법들은 제거하고, 해당 데이터만 가져와야 한다.

2️⃣ 훈련 데이터 전처리하기
- 정규표현식을 이용하여 훈련에 필요한 데이터만 정제

3️⃣ Word2Vec 훈련시키기
- Word2Vec의 하이퍼파라미터값은 다음과 같습니다.

vector_size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.

window = 컨텍스트 윈도우 크기

min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)

workers = 학습을 위한 프로세스 수

sg = 0은 CBOW, 1은 Skip-gram.

4️⃣ Word2Vec 모델 저장하고 로드하기
- 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장

##### 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개
자연어 처리 작업을 할때, 케라스의 Embedding()를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용 할 수도 있습니다.

예를 들어서 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황이라면, 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 때로는 더 좋은 성능을 얻을 수 있습니다.
